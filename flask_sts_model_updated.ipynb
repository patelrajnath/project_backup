{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"FLASK_DEBUG\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: flask in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask) (7.1.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n",
      "Requirement already satisfied: flask_restful in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: aniso8601>=0.82 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask_restful) (8.0.0)\n",
      "Requirement already satisfied: pytz in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask_restful) (2020.4)\n",
      "Requirement already satisfied: six>=1.3.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask_restful) (1.15.0)\n",
      "Requirement already satisfied: Flask>=0.8 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from flask_restful) (1.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from Flask>=0.8->flask_restful) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from Flask>=0.8->flask_restful) (2.11.2)\n",
      "Requirement already satisfied: click>=5.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from Flask>=0.8->flask_restful) (7.1.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from Flask>=0.8->flask_restful) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_restful) (1.1.1)\n",
      "Requirement already satisfied: pandas in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (1.1.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: gast==0.3.3 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.25.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (4.54.0)\n",
      "Requirement already satisfied: sklearn in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.0+cpu in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (1.7.0+cpu)\n",
      "Requirement already satisfied: torchvision==0.8.1+cpu in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (0.8.1+cpu)\n",
      "Requirement already satisfied: torchaudio===0.7.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch==1.7.0+cpu) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch==1.7.0+cpu) (0.6)\n",
      "Requirement already satisfied: future in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch==1.7.0+cpu) (0.18.2)\n",
      "Requirement already satisfied: numpy in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch==1.7.0+cpu) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torchvision==0.8.1+cpu) (8.0.1)\n",
      "Requirement already satisfied: laserembeddings in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from laserembeddings) (1.18.5)\n",
      "Requirement already satisfied: transliterate==1.10.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from laserembeddings) (1.10.2)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from laserembeddings) (0.0.35)\n",
      "Requirement already satisfied: subword-nmt<0.4.0,>=0.3.6 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from laserembeddings) (0.3.7)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from laserembeddings) (1.7.0+cpu)\n",
      "Requirement already satisfied: six>=1.1.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transliterate==1.10.2->laserembeddings) (1.15.0)\n",
      "Requirement already satisfied: joblib in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sacremoses==0.0.35->laserembeddings) (0.17.0)\n",
      "Requirement already satisfied: click in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sacremoses==0.0.35->laserembeddings) (7.1.2)\n",
      "Requirement already satisfied: tqdm in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sacremoses==0.0.35->laserembeddings) (4.54.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (0.6)\n",
      "Requirement already satisfied: future in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (0.18.2)\n",
      "Requirement already satisfied: sentence_transformers in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: scikit-learn in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (0.23.2)\n",
      "Requirement already satisfied: tqdm in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (4.54.0)\n",
      "Requirement already satisfied: transformers<3.6.0,>=3.1.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (3.5.1)\n",
      "Requirement already satisfied: numpy in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (1.18.5)\n",
      "Requirement already satisfied: scipy in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (1.5.4)\n",
      "Requirement already satisfied: nltk in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (3.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sentence_transformers) (1.7.0+cpu)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (0.17.0)\n",
      "Requirement already satisfied: requests in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (2.25.0)\n",
      "Requirement already satisfied: packaging in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (20.5)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (0.9.3)\n",
      "Requirement already satisfied: filelock in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (0.0.35)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (0.1.91)\n",
      "Requirement already satisfied: protobuf in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from transformers<3.6.0,>=3.1.0->sentence_transformers) (3.14.0)\n",
      "Requirement already satisfied: click in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from nltk->sentence_transformers) (7.1.2)\n",
      "Requirement already satisfied: dataclasses in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (0.6)\n",
      "Requirement already satisfied: typing-extensions in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
      "Requirement already satisfied: future in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (0.18.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from requests->transformers<3.6.0,>=3.1.0->sentence_transformers) (1.26.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: six in d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence_transformers) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models into D:\\PycharmProjects\\multilingual_classifier\\venv\\lib\\site-packages\\laserembeddings\\data\n",
      "\n",
      "   Downloading https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes...\n",
      "   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
      "   Downloading https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab...\n",
      "   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
      "   Downloading https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt...\n",
      "   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
      "\n",
      " You're all set!\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install flask\n",
    "!pip install flask_restful\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install tqdm\n",
    "!pip install sklearn\n",
    "!pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install laserembeddings\n",
    "!pip install sentence_transformers\n",
    "!python -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import typing\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional, Text, Tuple, Type\n",
    "import pickle\n",
    "import numpy as np\n",
    "####\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import json\n",
    "import abc\n",
    "import pandas as pd\n",
    "# import glog\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "from laserembeddings import Laser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "_TRAIN = \"train\"\n",
    "_TEST = \"test\"\n",
    "_VAL = 'validation'\n",
    "batchsize_super = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# from typing import Dict, Optional\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# _MAX_PER_BATCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if typing.TYPE_CHECKING:\n",
    "    import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingBatcher(collections.abc.Iterator):\n",
    "    \"\"\"Batcher that samples according to a given distribution.\n",
    "\n",
    "    It defaults to sampling from the data distribution.\n",
    "\n",
    "    WARNING: this class is not deterministic. if you want deterministic\n",
    "    behaviour, just freeze the numpy seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            examples: np.ndarray,\n",
    "            labels: np.ndarray,\n",
    "            batch_size: int,\n",
    "            sample_distribution: Optional[Dict[int, float]] = None,\n",
    "    ):\n",
    "        \"\"\"Create a new BalancedBatcher.\n",
    "\n",
    "        Args:\n",
    "            examples: np.ndarray containing examples\n",
    "            labels: np.ndarray containing labels\n",
    "            batch_size: int size of a single batch\n",
    "            sample_distribution: optional distribution over label\n",
    "                classes for sampling. This is normalized to sum to 1. Defines\n",
    "                the target distribution that batches will be sampled with.\n",
    "                Defaults to the data distribution.\n",
    "        \"\"\"\n",
    "        _validate_labels_examples(examples, labels)\n",
    "        self._examples = examples\n",
    "        self._labels = labels\n",
    "        self._label_classes = np.unique(labels)\n",
    "        self._class_to_indices = {\n",
    "            label: np.argwhere(labels == label).flatten()\n",
    "            for label in self._label_classes\n",
    "        }\n",
    "        if sample_distribution is None:\n",
    "            # Default to the data distribution\n",
    "            sample_distribution = {\n",
    "                label: float(indices.size)\n",
    "                for label, indices in self._class_to_indices.items()\n",
    "            }\n",
    "        self._label_choices, self._label_probs = (\n",
    "            self._get_label_choices_and_probs(sample_distribution))\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def _get_label_choices_and_probs(self, sample_distribution):\n",
    "        label_choices = []\n",
    "        label_probs = []\n",
    "        weight_sum = sum(sample_distribution.values())\n",
    "        for label, weight in sample_distribution.items():\n",
    "            if label not in self._labels:\n",
    "                raise ValueError(\n",
    "                    f\"label {label} in sample distribution does not exist\")\n",
    "            if weight < 0.0:\n",
    "                raise ValueError(\n",
    "                    f\"weight {weight} for label {label} is negative\")\n",
    "            label_choices.append(label)\n",
    "            label_probs.append(weight / weight_sum)\n",
    "\n",
    "        return np.array(label_choices), np.array(label_probs)\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Generates the next batch.\n",
    "\n",
    "        Returns:\n",
    "            (batch_of_examples, batch_of_labels) - a tuple of ndarrays\n",
    "        \"\"\"\n",
    "        class_choices = np.random.choice(\n",
    "            self._label_choices, size=self._batch_size, p=self._label_probs)\n",
    "\n",
    "        batch_indices = []\n",
    "        for class_choice in class_choices:\n",
    "            indices = self._class_to_indices[class_choice]\n",
    "            batch_indices.append(np.random.choice(indices))\n",
    "\n",
    "        return self._examples[batch_indices], self._labels[batch_indices]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Gets an iterator for this iterable\n",
    "\n",
    "        Returns:\n",
    "            self because the class is an iterator itself\n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationEncoderClient(object):\n",
    "    \"\"\"A model that maps from text to dense vectors.\"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encodes a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: a list of strings\n",
    "\n",
    "        Returns:\n",
    "            an (N, d) numpy matrix of sentence encodings.\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedEncoderClient(ClassificationEncoderClient):\n",
    "    \"\"\"concatenates the encodings of several ClassificationEncoderClients\n",
    "\n",
    "    Args:\n",
    "        encoders: A list of ClassificationEncoderClients\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoders: list):\n",
    "        \"\"\"constructor\"\"\"\n",
    "        self._encoders = encoders\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encode a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: the list of sentences\n",
    "\n",
    "        Returns:\n",
    "            an array with shape (len(sentences), ENCODING_SIZE)\n",
    "        \"\"\"\n",
    "        encodings = np.hstack([encoder.encode_sentences(sentences)\n",
    "                               for encoder in self._encoders])\n",
    "        print('DEBUG combined size:', encodings.shape)\n",
    "        return encodings\n",
    "\n",
    "\n",
    "\n",
    "class LaserEncoderClient(ClassificationEncoderClient):\n",
    "    \"\"\"A wrapper around ClassificationEncoderClient to normalise the output\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=100):\n",
    "        \"\"\"Create a new ConvertEncoderClient object\n",
    "\n",
    "        Args:\n",
    "            uri: The uri to the tensorflow_hub module\n",
    "            batch_size: maximum number of sentences to encode at once\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._encoder_client = Laser()\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encode a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: the list of sentences\n",
    "\n",
    "        Returns:\n",
    "            an (N, d) numpy matrix of sentence encodings.\n",
    "        \"\"\"\n",
    "        encodings = []\n",
    "        #         glog.setLevel(\"ERROR\")\n",
    "        for i in tqdm(range(0, len(sentences), self._batch_size),\n",
    "                      \"encoding sentence batches\"):\n",
    "            encodings.append(\n",
    "                self._encoder_client.embed_sentences(\n",
    "                    sentences[i:i + self._batch_size], lang='en'))\n",
    "        #         glog.setLevel(\"INFO\")\n",
    "        print('DEBUG LASER SIZE:', np.vstack(encodings).shape)\n",
    "        return l2_normalize(np.vstack(encodings))\n",
    "\n",
    "\n",
    "class SbertEncoderClient(ClassificationEncoderClient):\n",
    "    \"\"\"A wrapper around ClassificationEncoderClient to normalise the output\"\"\"\n",
    "\n",
    "    def __init__(self, sbert_model, batch_size=batchsize_super):\n",
    "        \"\"\"Create a new ConvertEncoderClient object\n",
    "\n",
    "        Args:\n",
    "            uri: The uri to the tensorflow_hub module\n",
    "            batch_size: maximum number of sentences to encode at once\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._encoder_client = SentenceTransformer(sbert_model)\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encode a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: the list of sentences\n",
    "\n",
    "        Returns:\n",
    "            an (N, d) numpy matrix of sentence encodings.\n",
    "        \"\"\"\n",
    "        encodings = []\n",
    "        #         glog.setLevel(\"ERROR\")\n",
    "        for i in tqdm(range(0, len(sentences), self._batch_size),\n",
    "                      \"encoding sentence batches\"):\n",
    "            encodings.append(\n",
    "                self._encoder_client.encode(\n",
    "                    sentences[i:i + self._batch_size]))\n",
    "        #         glog.setLevel(\"INFO\")\n",
    "        return l2_normalize(np.vstack(encodings))\n",
    "def l2_normalize(encodings):\n",
    "    \"\"\"L2 normalizes the given matrix of encodings.\"\"\"\n",
    "    norms = np.linalg.norm(encodings, ord=2, axis=-1, keepdims=True)\n",
    "    return encodings / norms\n",
    "\n",
    "\n",
    "class PolynomialDecay:\n",
    "    \"\"\"A callable that implements polynomial decay.\n",
    "\n",
    "    Used as a callback in keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_epochs, init_lr, power=1.0):\n",
    "        \"\"\"Creates a new PolynomialDecay\n",
    "\n",
    "        Args:\n",
    "            max_epochs: int, maximum number of epochs\n",
    "            init_lr: float, initial learning rate which will decay\n",
    "            power: float, the power of the decay function\n",
    "        \"\"\"\n",
    "        self.max_epochs = max_epochs\n",
    "        self.init_lr = init_lr\n",
    "        self.power = power\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"Calculates the new (smaller) learning rate for the current epoch\n",
    "\n",
    "        Args:\n",
    "            epoch: int, the epoch for which we need to calculate the LR\n",
    "\n",
    "        Returns:\n",
    "            float, the new learning rate\n",
    "        \"\"\"\n",
    "        decay = (1 - (epoch / float(self.max_epochs))) ** self.power\n",
    "        alpha = self.init_lr * decay\n",
    "\n",
    "        return float(alpha)\n",
    "\n",
    "\n",
    "def _create_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    dropout = hparams.dropout\n",
    "    optimizer_name = hparams.optimizer\n",
    "    optimizer = {\n",
    "        'adam': tf.keras.optimizers.Adam,\n",
    "        'sgd': tf.keras.optimizers.SGD\n",
    "    }[optimizer_name]\n",
    "\n",
    "    input_size = hparams.input_size\n",
    "    for _ in range(hparams.num_hidden_layers):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dropout(dropout, input_shape=(input_size,))\n",
    "        )\n",
    "        model.add(tf.keras.layers.Dense(hparams.hidden_layer_size,\n",
    "                                        activation=hparams.activation))\n",
    "        input_size = hparams.hidden_layer_size\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    model.compile(loss=\"mean_absolute_error\",\n",
    "                  optimizer=optimizer(lr=hparams.learning_rate),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def _validate_labels_examples(examples, labels):\n",
    "    if not isinstance(examples, np.ndarray):\n",
    "        raise ValueError(\"examples should be an ndarray\")\n",
    "\n",
    "    if not isinstance(labels, np.ndarray):\n",
    "        raise ValueError(\"labels should be ndarray\")\n",
    "\n",
    "    if not labels.size == examples.shape[0]:\n",
    "        raise ValueError(\"number of labels != number of examples\")\n",
    "\n",
    "\n",
    "def iter_to_generator(iterator):\n",
    "    \"\"\"Gets a generator from an iterator.\n",
    "\n",
    "    Used so that keras type checking does not complain.\n",
    "\n",
    "    Args:\n",
    "        iterator: any python iterator\n",
    "\n",
    "    Returns:\n",
    "        a python generator that just calls next on the iterator\n",
    "    \"\"\"\n",
    "\n",
    "    def gen():\n",
    "        while True:\n",
    "            yield next(iterator)\n",
    "\n",
    "    return gen()\n",
    "\n",
    "\n",
    "def train_model(train_encodings, train_labels, categories, hparams,\n",
    "                validation_data=None, verbose=1):\n",
    "    \"\"\"Trains an intent classification model\n",
    "\n",
    "    Args:\n",
    "        train_encodings: np.array with the train encodings\n",
    "        train_labels: list of labels corresponding to each train example\n",
    "        categories: the set of categories\n",
    "        hparams: a tf.contrib.training.HParams object containing the model\n",
    "            and training hyperparameters\n",
    "        validation_data: (validation_encodings, validation_labels) tuple\n",
    "        verbose: the keras_model.train() verbose level\n",
    "\n",
    "    Returns:\n",
    "        model: a keras model\n",
    "        eval_acc_history: The evaluation results per epoch\n",
    "\n",
    "    \"\"\"\n",
    "    distribution = None if not hparams.balance_data else {\n",
    "        x: 1. / len(categories) for x in range(len(categories))}\n",
    "\n",
    "    batcher = SamplingBatcher(\n",
    "        train_encodings, train_labels, hparams.batch_size, distribution)\n",
    "\n",
    "    steps_per_epoch = np.ceil(len(train_labels) / hparams.batch_size)\n",
    "\n",
    "    model, eval_acc_history = _train_mlp_with_generator(\n",
    "        batcher, train_encodings.shape[1], steps_per_epoch,\n",
    "        categories, hparams, validation_data=validation_data, verbose=verbose)\n",
    "    return model, eval_acc_history\n",
    "\n",
    "\n",
    "def _train_mlp_with_generator(\n",
    "        batcher, input_size, steps_per_epoch, label_set, hparams,\n",
    "        validation_data=None, verbose=1):\n",
    "    \"\"\"Trains a Multi Layer Perceptron (MLP) model using keras.\n",
    "\n",
    "    Args:\n",
    "        batcher: an instance of a class that inherits from abc.Iterator and\n",
    "            iterates through batches. see batchers.py for an example.\n",
    "        input_size: int, length of the input vector\n",
    "        steps_per_epoch: int, number of steps per one epoch\n",
    "        label_set: set of ints, the set of labels\n",
    "        hparams: an instance of tf.contrib.training.Hparams, see config.py\n",
    "            for some examples\n",
    "        validation_data: This can be either\n",
    "            - a generator for the validation data\n",
    "            - a tuple (inputs, targets)\n",
    "            - a tuple (inputs, targets, sample_weights).\n",
    "        verbose: keras verbosity mode, 0, 1, or 2.\n",
    "\n",
    "    Returns:\n",
    "        keras model, which has been trained\n",
    "        test accuracy history, as retreived from keras\n",
    "    \"\"\"\n",
    "\n",
    "    hparams.input_size = input_size\n",
    "    hparams.output_size = len(label_set)\n",
    "    #     print('!!!! validation data',validation_data)\n",
    "    model = _create_model(hparams)\n",
    "\n",
    "    callbacks = None\n",
    "    if hparams.lr_decay_pow:\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.LearningRateScheduler(PolynomialDecay(\n",
    "                max_epochs=hparams.epochs,\n",
    "                init_lr=hparams.learning_rate,\n",
    "                power=hparams.lr_decay_pow))]\n",
    "\n",
    "    #     glog.info(\"Training model...\")\n",
    "    history_callback = model.fit_generator(\n",
    "        generator=iter_to_generator(batcher),\n",
    "        steps_per_epoch=max(steps_per_epoch, 1),\n",
    "        epochs=hparams.epochs,\n",
    "        shuffle=False,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    test_acc_history = (None if not validation_data\n",
    "                        else history_callback.history[\"val_accuracy\"])\n",
    "\n",
    "    return model, test_acc_history\n",
    "\n",
    "\n",
    "class hparamset():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 32\n",
    "        self.balance_data = False\n",
    "        self.output_size = None\n",
    "        self.activation = 'relu'\n",
    "        self.hidden_layer_size = 512\n",
    "        self.num_hidden_layers = 1\n",
    "        self.batch_size = 32\n",
    "        self.dropout = 0.75\n",
    "        self.optimizer = 'sgd'\n",
    "        self.learning_rate = 0.7\n",
    "        self.lr_decay_pow = 1\n",
    "        self.epochs = 100\n",
    "        self.eval_each_epoch = True\n",
    "\n",
    "    ###############\n",
    "\n",
    "\n",
    "#############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = 'distiluse-base-multilingual-cased'\n",
    "sbert_model2 = 'xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "sbert_model3 = 'distilbert-multilingual-nli-stsb-quora-ranking'\n",
    "sbert_encoder = SbertEncoderClient(sbert_model)\n",
    "sbert_encoder2 = SbertEncoderClient(sbert_model2)\n",
    "sbert_encoder3 = SbertEncoderClient(sbert_model3)\n",
    "laser_encoder = LaserEncoderClient()\n",
    "encoder_client = CombinedEncoderClient([laser_encoder, sbert_encoder, sbert_encoder2, sbert_encoder3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(encoder_client, open('encoders_pickle.pkl','wb') )\n",
    "# encoder_client = pickle.load(open('encoders_pickle.pkl','rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_restful import Resource, Api, reqparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "api = Api(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global model\n",
    "# model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "class FlaskServingModel(Resource):\n",
    "    def __init__(self):\n",
    "        self._required_features = ['text', 'labels', 'unique_labels']\n",
    "        self.reqparse = reqparse.RequestParser()\n",
    "        for feature in self._required_features:\n",
    "            self.reqparse.add_argument(\n",
    "                feature, type = list, required = True, location = 'json',\n",
    "                help = 'No {} provided'.format(feature))\n",
    "        self.hparams = hparamset()\n",
    "        self.verbose = 0\n",
    "        self.encodings = {}\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.training_text  = []\n",
    "        self.categories = {}\n",
    "        self.eval_acc_history = None\n",
    "        self.test_text = None\n",
    "        print('INITIALISED')\n",
    "        super(FlaskServingModel, self).__init__()\n",
    "        \n",
    "    def put(self):\n",
    "        \"\"\"\n",
    "        TRAIN METHOD\n",
    "        \"\"\"\n",
    "        args = self.reqparse.parse_args()        \n",
    "        self.training_text = args['text']\n",
    "        self.y = np.array(args['labels'])\n",
    "        self.categories = args['unique_labels']\n",
    "        \n",
    "        enc = encoder_client.encode_sentences(self.training_text)#train.text.values[:100])#\n",
    "        global model\n",
    "        model, eval_acc_history = train_model(enc, self.y, self.categories, self.hparams, validation_data=None,verbose=0)\n",
    "        model.save('trained_model.tf')\n",
    "        print('model trained AND saved')\n",
    "        return {'status':'trained and saved'}   \n",
    "    def post(self):\n",
    "        \"\"\"\n",
    "        PREDICTION METHOD\n",
    "        \"\"\"\n",
    "        args = self.reqparse.parse_args()        \n",
    "        enc = encoder_client.encode_sentences(args['text']) \n",
    "        print(f'inferring on: {args[\"text\"]}')\n",
    "        global model\n",
    "        pred = model.predict(enc) #enc[:100])\n",
    "        print(pred)\n",
    "        return {'prediction':pred.tolist()}\n",
    "    \n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        EVALUATE\n",
    "        \"\"\"\n",
    "        args = self.reqparse.parse_args()\n",
    "        self.test_text = encoder_client.encode_sentences(args['text']) \n",
    "        global model\n",
    "        out = model.predict(self.test_text)\n",
    "        prediction = np.argmax(out, axis=1)\n",
    "        s= pd.DataFrame({'pred':prediction,'testlabels':args['labels']})#, 'language':test.language})#.to_csv('xlni_preds_temp.csv') #lang\n",
    "        ax_test_acc = accuracy_score(s.testlabels, s.pred)\n",
    "        return {'accuracy':ax_test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = FlaskServingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/polyai-models/data/polyai-models/200909_test_huawei_wallet_30strat_min3_manyanswers.csv')\n",
    "# train = pd.read_csv('/polyai-models/data/polyai-models/200909_train_huawei_wallet_60strat_min3_manyanswers.csv')\n",
    "# val = pd.read_csv('/polyai-models/data/polyai-models/200909_val_huawei_wallet_10strat_min3_manyanswers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.put()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.add_resource(FlaskServingModel, '/train', '/predict', '/evaluate')\n",
    "# api.add_resource(FlaskServingModel, '/train')\n",
    "# api.add_resource(FlaskServingModel, '/predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://127.0.0.1:9501/ (Press CTRL+C to quit)\n",
      "encoding sentence batches:   0%|                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "encoding sentence batches:   0%|                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG LASER SIZE: (89, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.43it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.42s/it]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG combined size: (89, 3072)\n",
      "WARNING:tensorflow:From <ipython-input-9-faa39cd4e4ca>:266: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-faa39cd4e4ca>:266: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model.tf\\assets\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:42:21] \"\u001b[37mPUT /train HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained AND saved\n",
      "INITIALISED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:42:25] \"\u001b[35m\u001b[1mPOST /predict HTTP/1.1\u001b[0m\" 500 -\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 2464, in __call__\n",
      "    return self.wsgi_app(environ, start_response)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 2450, in wsgi_app\n",
      "    response = self.handle_exception(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 272, in error_router\n",
      "    return original_handler(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1867, in handle_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\_compat.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 272, in error_router\n",
      "    return original_handler(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\_compat.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 468, in wrapper\n",
      "    resp = resource(*args, **kwargs)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\views.py\", line 89, in view\n",
      "    return self.dispatch_request(*args, **kwargs)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 583, in dispatch_request\n",
      "    resp = meth(*args, **kwargs)\n",
      "  File \"<ipython-input-15-1ca78c58fdc1>\", line 42, in post\n",
      "    \n",
      "  File \"<ipython-input-9-faa39cd4e4ca>\", line 22, in encode_sentences\n",
      "    \n",
      "  File \"<ipython-input-9-faa39cd4e4ca>\", line 22, in <listcomp>\n",
      "    \n",
      "  File \"<ipython-input-9-faa39cd4e4ca>\", line 52, in encode_sentences\n",
      "    \n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "encoding sentence batches:   0%|                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "encoding sentence batches:   0%|                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG LASER SIZE: (89, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.07it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG combined size: (89, 3072)\n",
      "INFO:tensorflow:Assets written to: trained_model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model.tf\\assets\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:46:40] \"\u001b[37mPUT /train HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained AND saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:46:44] \"\u001b[35m\u001b[1mPOST /predict HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 2464, in __call__\n",
      "    return self.wsgi_app(environ, start_response)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 2450, in wsgi_app\n",
      "    response = self.handle_exception(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 272, in error_router\n",
      "    return original_handler(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1867, in handle_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\_compat.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 272, in error_router\n",
      "    return original_handler(e)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\_compat.py\", line 38, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 468, in wrapper\n",
      "    resp = resource(*args, **kwargs)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask\\views.py\", line 89, in view\n",
      "    return self.dispatch_request(*args, **kwargs)\n",
      "  File \"d:\\pycharmprojects\\multilingual_classifier\\venv\\lib\\site-packages\\flask_restful\\__init__.py\", line 583, in dispatch_request\n",
      "    resp = meth(*args, **kwargs)\n",
      "  File \"<ipython-input-15-1ca78c58fdc1>\", line 42, in post\n",
      "    \n",
      "  File \"<ipython-input-9-faa39cd4e4ca>\", line 22, in encode_sentences\n",
      "    \n",
      "  File \"<ipython-input-9-faa39cd4e4ca>\", line 22, in <listcomp>\n",
      "    \n",
      "  File \"<ipython-input-9-faa39cd4e4ca>\", line 52, in encode_sentences\n",
      "    \n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "encoding sentence batches:   0%|                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "encoding sentence batches:   0%|                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG LASER SIZE: (89, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.36it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG combined size: (89, 3072)\n",
      "INFO:tensorflow:Assets written to: trained_model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model.tf\\assets\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:51:42] \"\u001b[37mPUT /train HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained AND saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.41it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.86it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n",
      "DEBUG LASER SIZE: (3, 1024)\n",
      "DEBUG combined size: (3, 3072)\n",
      "inferring on: ['H', 'e', 'y']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:52:46] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.12136135e-01 7.86891021e-03 8.16332642e-03 1.58567548e-01\n",
      "  8.89443681e-02 4.86943901e-01 6.53283810e-03 3.08429617e-02]\n",
      " [8.71386170e-01 9.73247050e-04 9.57908342e-04 6.40936270e-02\n",
      "  1.72142256e-02 2.56461315e-02 2.18581641e-03 1.75427608e-02]\n",
      " [9.25942242e-01 7.81029812e-04 8.38064123e-04 3.13457958e-02\n",
      "  1.28497835e-02 1.56979747e-02 1.57436659e-03 1.09709213e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.47it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.31it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.15it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n",
      "DEBUG LASER SIZE: (5, 1024)\n",
      "DEBUG combined size: (5, 3072)\n",
      "inferring on: ['H', 'e', 'l', 'l', 'o']\n",
      "[[0.21213613 0.00786891 0.00816333 0.15856759 0.08894439 0.48694378\n",
      "  0.00653284 0.03084297]\n",
      " [0.8713862  0.00097325 0.00095791 0.06409366 0.01721423 0.02564614\n",
      "  0.00218582 0.01754277]\n",
      " [0.52986515 0.00381286 0.00347785 0.22425166 0.06166773 0.09919198\n",
      "  0.00704514 0.0706876 ]\n",
      " [0.52986515 0.00381286 0.00347785 0.22425166 0.06166773 0.09919198\n",
      "  0.00704514 0.0706876 ]\n",
      " [0.58755946 0.00288869 0.00327609 0.14252663 0.03836206 0.19087933\n",
      "  0.00248561 0.03202212]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:52:49] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.47it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.51it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n",
      "DEBUG LASER SIZE: (12, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:52:55] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG combined size: (12, 3072)\n",
      "inferring on: ['H', 'o', 'w', ' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u', '?']\n",
      "[[2.1213613e-01 7.8689121e-03 8.1633320e-03 1.5856759e-01 8.8944390e-02\n",
      "  4.8694378e-01 6.5328395e-03 3.0842969e-02]\n",
      " [5.8755934e-01 2.8886942e-03 3.2760878e-03 1.4252669e-01 3.8362060e-02\n",
      "  1.9087939e-01 2.4856059e-03 3.2022119e-02]\n",
      " [2.2774069e-01 7.1952697e-03 6.7250994e-03 2.3231927e-01 1.8612449e-01\n",
      "  2.7083510e-01 6.4712805e-03 6.2588781e-02]\n",
      " [5.7118630e-01 3.8775357e-03 4.1034226e-03 1.7053838e-01 5.8352832e-02\n",
      "  5.3972613e-02 4.0568849e-03 1.3391191e-01]\n",
      " [6.2336302e-01 2.9429777e-03 3.4515695e-03 1.6951038e-01 8.9116074e-02\n",
      "  7.5608701e-02 3.3230446e-03 3.2684226e-02]\n",
      " [2.7584463e-01 9.6707093e-03 9.6074874e-03 2.4118851e-01 1.6060558e-01\n",
      "  2.2172132e-01 1.4144824e-02 6.7216963e-02]\n",
      " [8.7138617e-01 9.7324705e-04 9.5790881e-04 6.4093664e-02 1.7214231e-02\n",
      "  2.5646145e-02 2.1858176e-03 1.7542768e-02]\n",
      " [5.7118630e-01 3.8775357e-03 4.1034226e-03 1.7053838e-01 5.8352832e-02\n",
      "  5.3972613e-02 4.0568849e-03 1.3391191e-01]\n",
      " [9.2594206e-01 7.8102929e-04 8.3806401e-04 3.1345803e-02 1.2849781e-02\n",
      "  1.5697973e-02 1.5743655e-03 1.0970924e-02]\n",
      " [5.8755934e-01 2.8886942e-03 3.2760878e-03 1.4252669e-01 3.8362060e-02\n",
      "  1.9087939e-01 2.4856059e-03 3.2022119e-02]\n",
      " [1.8954736e-01 5.2467622e-03 7.0272367e-03 1.6739792e-01 1.6089042e-01\n",
      "  4.2042845e-01 2.7605284e-03 4.6701308e-02]\n",
      " [2.3704110e-01 2.0591458e-02 2.6737306e-02 5.2629793e-01 2.3962475e-02\n",
      "  8.8045940e-02 1.2753657e-03 7.6048397e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.48it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.27it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.86it/s]\n",
      "encoding sentence batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 32.26it/s]\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Nov/2020 23:53:00] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISED\n",
      "DEBUG LASER SIZE: (7, 1024)\n",
      "DEBUG combined size: (7, 3072)\n",
      "inferring on: ['B', 'o', 'n', 'j', 'o', 'u', 'r']\n",
      "[[3.75780731e-01 1.05415285e-02 9.55444388e-03 2.39334702e-01\n",
      "  1.46029800e-01 1.63915664e-01 6.48473995e-03 4.83583957e-02]\n",
      " [5.87559342e-01 2.88869417e-03 3.27608781e-03 1.42526686e-01\n",
      "  3.83620597e-02 1.90879390e-01 2.48560589e-03 3.20221186e-02]\n",
      " [2.94192526e-02 1.23094989e-03 1.58568402e-03 9.32311177e-01\n",
      "  7.19654793e-03 1.54104978e-02 2.03080999e-04 1.26428325e-02]\n",
      " [2.14024559e-01 6.88460842e-03 9.73494444e-03 1.87073007e-01\n",
      "  1.49131611e-01 4.02556121e-01 3.85217112e-03 2.67430339e-02]\n",
      " [5.87559342e-01 2.88869417e-03 3.27608781e-03 1.42526686e-01\n",
      "  3.83620597e-02 1.90879390e-01 2.48560589e-03 3.20221186e-02]\n",
      " [1.89547360e-01 5.24676358e-03 7.02723907e-03 1.67397931e-01\n",
      "  1.60890400e-01 4.20428365e-01 2.76052789e-03 4.67013121e-02]\n",
      " [2.75844663e-01 9.67071019e-03 9.60748922e-03 2.41188526e-01\n",
      "  1.60605595e-01 2.21721277e-01 1.41448248e-02 6.72169700e-02]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,  port=9501, use_reloader=False)#host='0.0.0.0',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = encoder_client.encode_sentences(train.text.values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, eval_acc_history = train_model(enc, train.labels.values[:100], categories=train.labels.unique(), hparams=hparamset(), validation_data=None,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.predict(enc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('test.tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=load_model('test.tr')\n",
    "# model = load_model('trained_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(enc[:100]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mod.post({'text':train.text.values, 'labels':train.labels.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # args = self.reqparse.parse_args()\n",
    "# enc = encoder_client.encode_sentences(train.text.values[:100]) \n",
    "# # print(f'inferring on: {args[\"text\"]}')\n",
    "# model = load_model('trained_model.tf')\n",
    "# print('MODEL LOADED')\n",
    "# pred = model.predict(enc)\n",
    "# print(pred)\n",
    "# # return {'prediction':pred.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
