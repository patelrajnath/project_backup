{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"FLASK_DEBUG\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models into D:\\PycharmProjects\\multilingual_classifier\\venv\\lib\\site-packages\\laserembeddings\\data\n",
      "\n",
      "   Downloading https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes...\n",
      "   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
      "   Downloading https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab...\n",
      "   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
      "   Downloading https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt...\n",
      "   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
      "\n",
      " You're all set!\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "# !pip install flask\n",
    "# !pip install flask_restful\n",
    "# !pip install pandas\n",
    "# !pip install tensorflow\n",
    "# !pip install tqdm\n",
    "# !pip install sklearn\n",
    "# !pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install laserembeddings\n",
    "# !pip install sentence_transformers\n",
    "# !python -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import typing\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional, Text, Tuple, Type\n",
    "import pickle\n",
    "import numpy as np\n",
    "####\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import json\n",
    "import abc\n",
    "import pandas as pd\n",
    "# import glog\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "from laserembeddings import Laser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "_TRAIN = \"train\"\n",
    "_TEST = \"test\"\n",
    "_VAL = 'validation'\n",
    "batchsize_super = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# from typing import Dict, Optional\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# _MAX_PER_BATCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if typing.TYPE_CHECKING:\n",
    "    import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingBatcher(collections.abc.Iterator):\n",
    "    \"\"\"Batcher that samples according to a given distribution.\n",
    "\n",
    "    It defaults to sampling from the data distribution.\n",
    "\n",
    "    WARNING: this class is not deterministic. if you want deterministic\n",
    "    behaviour, just freeze the numpy seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            examples: np.ndarray,\n",
    "            labels: np.ndarray,\n",
    "            batch_size: int,\n",
    "            sample_distribution: Optional[Dict[int, float]] = None,\n",
    "    ):\n",
    "        \"\"\"Create a new BalancedBatcher.\n",
    "\n",
    "        Args:\n",
    "            examples: np.ndarray containing examples\n",
    "            labels: np.ndarray containing labels\n",
    "            batch_size: int size of a single batch\n",
    "            sample_distribution: optional distribution over label\n",
    "                classes for sampling. This is normalized to sum to 1. Defines\n",
    "                the target distribution that batches will be sampled with.\n",
    "                Defaults to the data distribution.\n",
    "        \"\"\"\n",
    "        _validate_labels_examples(examples, labels)\n",
    "        self._examples = examples\n",
    "        self._labels = labels\n",
    "        self._label_classes = np.unique(labels)\n",
    "        self._class_to_indices = {\n",
    "            label: np.argwhere(labels == label).flatten()\n",
    "            for label in self._label_classes\n",
    "        }\n",
    "        if sample_distribution is None:\n",
    "            # Default to the data distribution\n",
    "            sample_distribution = {\n",
    "                label: float(indices.size)\n",
    "                for label, indices in self._class_to_indices.items()\n",
    "            }\n",
    "        self._label_choices, self._label_probs = (\n",
    "            self._get_label_choices_and_probs(sample_distribution))\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def _get_label_choices_and_probs(self, sample_distribution):\n",
    "        label_choices = []\n",
    "        label_probs = []\n",
    "        weight_sum = sum(sample_distribution.values())\n",
    "        for label, weight in sample_distribution.items():\n",
    "            if label not in self._labels:\n",
    "                raise ValueError(\n",
    "                    f\"label {label} in sample distribution does not exist\")\n",
    "            if weight < 0.0:\n",
    "                raise ValueError(\n",
    "                    f\"weight {weight} for label {label} is negative\")\n",
    "            label_choices.append(label)\n",
    "            label_probs.append(weight / weight_sum)\n",
    "\n",
    "        return np.array(label_choices), np.array(label_probs)\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Generates the next batch.\n",
    "\n",
    "        Returns:\n",
    "            (batch_of_examples, batch_of_labels) - a tuple of ndarrays\n",
    "        \"\"\"\n",
    "        class_choices = np.random.choice(\n",
    "            self._label_choices, size=self._batch_size, p=self._label_probs)\n",
    "\n",
    "        batch_indices = []\n",
    "        for class_choice in class_choices:\n",
    "            indices = self._class_to_indices[class_choice]\n",
    "            batch_indices.append(np.random.choice(indices))\n",
    "\n",
    "        return self._examples[batch_indices], self._labels[batch_indices]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Gets an iterator for this iterable\n",
    "\n",
    "        Returns:\n",
    "            self because the class is an iterator itself\n",
    "        \"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationEncoderClient(object):\n",
    "    \"\"\"A model that maps from text to dense vectors.\"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encodes a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: a list of strings\n",
    "\n",
    "        Returns:\n",
    "            an (N, d) numpy matrix of sentence encodings.\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedEncoderClient(ClassificationEncoderClient):\n",
    "    \"\"\"concatenates the encodings of several ClassificationEncoderClients\n",
    "\n",
    "    Args:\n",
    "        encoders: A list of ClassificationEncoderClients\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoders: list):\n",
    "        \"\"\"constructor\"\"\"\n",
    "        self._encoders = encoders\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encode a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: the list of sentences\n",
    "\n",
    "        Returns:\n",
    "            an array with shape (len(sentences), ENCODING_SIZE)\n",
    "        \"\"\"\n",
    "        encodings = np.hstack([encoder.encode_sentences(sentences)\n",
    "                               for encoder in self._encoders])\n",
    "        print('DEBUG combined size:', encodings.shape)\n",
    "        return encodings\n",
    "\n",
    "\n",
    "\n",
    "class LaserEncoderClient(ClassificationEncoderClient):\n",
    "    \"\"\"A wrapper around ClassificationEncoderClient to normalise the output\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=100):\n",
    "        \"\"\"Create a new ConvertEncoderClient object\n",
    "\n",
    "        Args:\n",
    "            uri: The uri to the tensorflow_hub module\n",
    "            batch_size: maximum number of sentences to encode at once\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._encoder_client = Laser()\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encode a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: the list of sentences\n",
    "\n",
    "        Returns:\n",
    "            an (N, d) numpy matrix of sentence encodings.\n",
    "        \"\"\"\n",
    "        encodings = []\n",
    "        #         glog.setLevel(\"ERROR\")\n",
    "        for i in tqdm(range(0, len(sentences), self._batch_size),\n",
    "                      \"encoding sentence batches\"):\n",
    "            encodings.append(\n",
    "                self._encoder_client.embed_sentences(\n",
    "                    sentences[i:i + self._batch_size], lang='en'))\n",
    "        #         glog.setLevel(\"INFO\")\n",
    "        print('DEBUG LASER SIZE:', np.vstack(encodings).shape)\n",
    "        return l2_normalize(np.vstack(encodings))\n",
    "\n",
    "\n",
    "class SbertEncoderClient(ClassificationEncoderClient):\n",
    "    \"\"\"A wrapper around ClassificationEncoderClient to normalise the output\"\"\"\n",
    "\n",
    "    def __init__(self, sbert_model, batch_size=batchsize_super):\n",
    "        \"\"\"Create a new ConvertEncoderClient object\n",
    "\n",
    "        Args:\n",
    "            uri: The uri to the tensorflow_hub module\n",
    "            batch_size: maximum number of sentences to encode at once\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._encoder_client = SentenceTransformer(sbert_model)\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        \"\"\"Encode a list of sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: the list of sentences\n",
    "\n",
    "        Returns:\n",
    "            an (N, d) numpy matrix of sentence encodings.\n",
    "        \"\"\"\n",
    "        encodings = []\n",
    "        #         glog.setLevel(\"ERROR\")\n",
    "        for i in tqdm(range(0, len(sentences), self._batch_size),\n",
    "                      \"encoding sentence batches\"):\n",
    "            encodings.append(\n",
    "                self._encoder_client.encode(\n",
    "                    sentences[i:i + self._batch_size]))\n",
    "        #         glog.setLevel(\"INFO\")\n",
    "        return l2_normalize(np.vstack(encodings))\n",
    "def l2_normalize(encodings):\n",
    "    \"\"\"L2 normalizes the given matrix of encodings.\"\"\"\n",
    "    norms = np.linalg.norm(encodings, ord=2, axis=-1, keepdims=True)\n",
    "    return encodings / norms\n",
    "\n",
    "\n",
    "class PolynomialDecay:\n",
    "    \"\"\"A callable that implements polynomial decay.\n",
    "\n",
    "    Used as a callback in keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_epochs, init_lr, power=1.0):\n",
    "        \"\"\"Creates a new PolynomialDecay\n",
    "\n",
    "        Args:\n",
    "            max_epochs: int, maximum number of epochs\n",
    "            init_lr: float, initial learning rate which will decay\n",
    "            power: float, the power of the decay function\n",
    "        \"\"\"\n",
    "        self.max_epochs = max_epochs\n",
    "        self.init_lr = init_lr\n",
    "        self.power = power\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"Calculates the new (smaller) learning rate for the current epoch\n",
    "\n",
    "        Args:\n",
    "            epoch: int, the epoch for which we need to calculate the LR\n",
    "\n",
    "        Returns:\n",
    "            float, the new learning rate\n",
    "        \"\"\"\n",
    "        decay = (1 - (epoch / float(self.max_epochs))) ** self.power\n",
    "        alpha = self.init_lr * decay\n",
    "\n",
    "        return float(alpha)\n",
    "\n",
    "\n",
    "def _create_model(hparams):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    dropout = hparams.dropout\n",
    "    optimizer_name = hparams.optimizer\n",
    "    optimizer = {\n",
    "        'adam': tf.keras.optimizers.Adam,\n",
    "        'sgd': tf.keras.optimizers.SGD\n",
    "    }[optimizer_name]\n",
    "\n",
    "    input_size = hparams.input_size\n",
    "    for _ in range(hparams.num_hidden_layers):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dropout(dropout, input_shape=(input_size,))\n",
    "        )\n",
    "        model.add(tf.keras.layers.Dense(hparams.hidden_layer_size,\n",
    "                                        activation=hparams.activation))\n",
    "        input_size = hparams.hidden_layer_size\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(hparams.output_size, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=optimizer(lr=hparams.learning_rate),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def _validate_labels_examples(examples, labels):\n",
    "    if not isinstance(examples, np.ndarray):\n",
    "        raise ValueError(\"examples should be an ndarray\")\n",
    "\n",
    "    if not isinstance(labels, np.ndarray):\n",
    "        raise ValueError(\"labels should be ndarray\")\n",
    "\n",
    "    if not labels.size == examples.shape[0]:\n",
    "        raise ValueError(\"number of labels != number of examples\")\n",
    "\n",
    "\n",
    "def iter_to_generator(iterator):\n",
    "    \"\"\"Gets a generator from an iterator.\n",
    "\n",
    "    Used so that keras type checking does not complain.\n",
    "\n",
    "    Args:\n",
    "        iterator: any python iterator\n",
    "\n",
    "    Returns:\n",
    "        a python generator that just calls next on the iterator\n",
    "    \"\"\"\n",
    "\n",
    "    def gen():\n",
    "        while True:\n",
    "            yield next(iterator)\n",
    "\n",
    "    return gen()\n",
    "\n",
    "\n",
    "def train_model(train_encodings, train_labels, categories, hparams,\n",
    "                validation_data=None, verbose=1):\n",
    "    \"\"\"Trains an intent classification model\n",
    "\n",
    "    Args:\n",
    "        train_encodings: np.array with the train encodings\n",
    "        train_labels: list of labels corresponding to each train example\n",
    "        categories: the set of categories\n",
    "        hparams: a tf.contrib.training.HParams object containing the model\n",
    "            and training hyperparameters\n",
    "        validation_data: (validation_encodings, validation_labels) tuple\n",
    "        verbose: the keras_model.train() verbose level\n",
    "\n",
    "    Returns:\n",
    "        model: a keras model\n",
    "        eval_acc_history: The evaluation results per epoch\n",
    "\n",
    "    \"\"\"\n",
    "    distribution = None if not hparams.balance_data else {\n",
    "        x: 1. / len(categories) for x in range(len(categories))}\n",
    "\n",
    "    batcher = SamplingBatcher(\n",
    "        train_encodings, train_labels, hparams.batch_size, distribution)\n",
    "\n",
    "    steps_per_epoch = np.ceil(len(train_labels) / hparams.batch_size)\n",
    "\n",
    "    model, eval_acc_history = _train_mlp_with_generator(\n",
    "        batcher, train_encodings.shape[1], steps_per_epoch,\n",
    "        categories, hparams, validation_data=validation_data, verbose=verbose)\n",
    "    return model, eval_acc_history\n",
    "\n",
    "\n",
    "def _train_mlp_with_generator(\n",
    "        batcher, input_size, steps_per_epoch, label_set, hparams,\n",
    "        validation_data=None, verbose=1):\n",
    "    \"\"\"Trains a Multi Layer Perceptron (MLP) model using keras.\n",
    "\n",
    "    Args:\n",
    "        batcher: an instance of a class that inherits from abc.Iterator and\n",
    "            iterates through batches. see batchers.py for an example.\n",
    "        input_size: int, length of the input vector\n",
    "        steps_per_epoch: int, number of steps per one epoch\n",
    "        label_set: set of ints, the set of labels\n",
    "        hparams: an instance of tf.contrib.training.Hparams, see config.py\n",
    "            for some examples\n",
    "        validation_data: This can be either\n",
    "            - a generator for the validation data\n",
    "            - a tuple (inputs, targets)\n",
    "            - a tuple (inputs, targets, sample_weights).\n",
    "        verbose: keras verbosity mode, 0, 1, or 2.\n",
    "\n",
    "    Returns:\n",
    "        keras model, which has been trained\n",
    "        test accuracy history, as retreived from keras\n",
    "    \"\"\"\n",
    "\n",
    "    hparams.input_size = input_size\n",
    "    hparams.output_size = len(label_set)\n",
    "    #     print('!!!! validation data',validation_data)\n",
    "    model = _create_model(hparams)\n",
    "\n",
    "    callbacks = None\n",
    "    if hparams.lr_decay_pow:\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.LearningRateScheduler(PolynomialDecay(\n",
    "                max_epochs=hparams.epochs,\n",
    "                init_lr=hparams.learning_rate,\n",
    "                power=hparams.lr_decay_pow))]\n",
    "\n",
    "    #     glog.info(\"Training model...\")\n",
    "    history_callback = model.fit_generator(\n",
    "        generator=iter_to_generator(batcher),\n",
    "        steps_per_epoch=max(steps_per_epoch, 1),\n",
    "        epochs=hparams.epochs,\n",
    "        shuffle=False,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    test_acc_history = (None if not validation_data\n",
    "                        else history_callback.history[\"val_accuracy\"])\n",
    "\n",
    "    return model, test_acc_history\n",
    "\n",
    "\n",
    "class hparamset():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 32\n",
    "        self.balance_data = False\n",
    "        self.output_size = None\n",
    "        self.activation = 'relu'\n",
    "        self.hidden_layer_size = 512\n",
    "        self.num_hidden_layers = 1\n",
    "        self.batch_size = 32\n",
    "        self.dropout = 0.75\n",
    "        self.optimizer = 'sgd'\n",
    "        self.learning_rate = 0.7\n",
    "        self.lr_decay_pow = 1\n",
    "        self.epochs = 100\n",
    "        self.eval_each_epoch = True\n",
    "\n",
    "    ###############\n",
    "\n",
    "\n",
    "#############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = 'distiluse-base-multilingual-cased'\n",
    "sbert_model2 = 'xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "sbert_model3 = 'distilbert-multilingual-nli-stsb-quora-ranking'\n",
    "sbert_encoder = SbertEncoderClient(sbert_model)\n",
    "sbert_encoder2 = SbertEncoderClient(sbert_model2)\n",
    "sbert_encoder3 = SbertEncoderClient(sbert_model3)\n",
    "laser_encoder = LaserEncoderClient()\n",
    "encoder_client = CombinedEncoderClient([laser_encoder, sbert_encoder, sbert_encoder2, sbert_encoder3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(encoder_client, open('encoders_pickle.pkl','wb') )\n",
    "# encoder_client = pickle.load(open('encoders_pickle.pkl','rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_restful import Resource, Api, reqparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "api = Api(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global model\n",
    "# model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "class FlaskServingModel(Resource):\n",
    "    def __init__(self):\n",
    "        self._required_features = ['text', 'labels', 'unique_labels']\n",
    "        self.reqparse = reqparse.RequestParser()\n",
    "        for feature in self._required_features:\n",
    "            self.reqparse.add_argument(\n",
    "                feature, type = list, required = True, location = 'json',\n",
    "                help = 'No {} provided'.format(feature))\n",
    "        self.hparams = hparamset()\n",
    "        self.verbose = 0\n",
    "        self.encodings = {}\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.training_text  = []\n",
    "        self.categories = {}\n",
    "        self.eval_acc_history = None\n",
    "        self.test_text = None\n",
    "        print('INITIALISED')\n",
    "        super(FlaskServingModel, self).__init__()\n",
    "        \n",
    "    def put(self):\n",
    "        \"\"\"\n",
    "        TRAIN METHOD\n",
    "        \"\"\"\n",
    "        args = self.reqparse.parse_args()        \n",
    "        self.training_text = args['text']\n",
    "        self.y = np.array(args['labels'])\n",
    "        self.categories = args['unique_labels']\n",
    "        \n",
    "        enc = encoder_client.encode_sentences(self.training_text)#train.text.values[:100])#\n",
    "        global model\n",
    "        model, eval_acc_history = train_model(enc, self.y, self.categories, self.hparams, validation_data=None,verbose=0)\n",
    "        model.save('trained_model.tf')\n",
    "        print('model trained AND saved')\n",
    "        return {'status':'trained and saved'}   \n",
    "    def post(self):\n",
    "        \"\"\"\n",
    "        PREDICTION METHOD\n",
    "        \"\"\"\n",
    "        args = self.reqparse.parse_args()        \n",
    "        enc = encoder_client.encode_sentences(args['text']) \n",
    "        print(f'inferring on: {args[\"text\"]}')\n",
    "        global model\n",
    "        pred = model.predict(enc) #enc[:100])\n",
    "        return {'prediction':pred.tolist()}\n",
    "    \n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        EVALUATE\n",
    "        \"\"\"\n",
    "        args = self.reqparse.parse_args()\n",
    "        self.test_text = encoder_client.encode_sentences(args['text']) \n",
    "        global model\n",
    "        out = model.predict(self.test_text)\n",
    "        prediction = np.argmax(out, axis=1)\n",
    "        s= pd.DataFrame({'pred':prediction,'testlabels':args['labels']})#, 'language':test.language})#.to_csv('xlni_preds_temp.csv') #lang\n",
    "        ax_test_acc = accuracy_score(s.testlabels, s.pred)\n",
    "        return {'accuracy':ax_test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = FlaskServingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/polyai-models/data/polyai-models/200909_test_huawei_wallet_30strat_min3_manyanswers.csv')\n",
    "# train = pd.read_csv('/polyai-models/data/polyai-models/200909_train_huawei_wallet_60strat_min3_manyanswers.csv')\n",
    "# val = pd.read_csv('/polyai-models/data/polyai-models/200909_val_huawei_wallet_10strat_min3_manyanswers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.put()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.add_resource(FlaskServingModel, '/train', '/predict', '/evaluate')\n",
    "# api.add_resource(FlaskServingModel, '/train')\n",
    "# api.add_resource(FlaskServingModel, '/predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,  port=9501, use_reloader=False)#host='0.0.0.0',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = encoder_client.encode_sentences(train.text.values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, eval_acc_history = train_model(enc, train.labels.values[:100], categories=train.labels.unique(), hparams=hparamset(), validation_data=None,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.predict(enc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('test.tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=load_model('test.tr')\n",
    "# model = load_model('trained_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(enc[:100]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mod.post({'text':train.text.values, 'labels':train.labels.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # args = self.reqparse.parse_args()\n",
    "# enc = encoder_client.encode_sentences(train.text.values[:100]) \n",
    "# # print(f'inferring on: {args[\"text\"]}')\n",
    "# model = load_model('trained_model.tf')\n",
    "# print('MODEL LOADED')\n",
    "# pred = model.predict(enc)\n",
    "# print(pred)\n",
    "# # return {'prediction':pred.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
